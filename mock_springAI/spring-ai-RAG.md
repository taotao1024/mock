# [spring-ai-graph](spring-ai-graph)
大模型Mock启动前，需要先设置环境变量。
```AI_DASHSCOPE_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx```

# RAG(Retrieval-augmented Generation)检索增强生成
## 大模型的幻觉
大模型生成的内容，受概率生成机制的影响。生成的下一个字依赖于统计概率预测结果中最有可能的词。

基于Transformer架构的模型还无法彻底杜绝AI幻觉，可以通过给大模型外挂知识库补充大模型知识的短板。这一行为被称为RAG(检索增强生成)

RAG通过将LLM与外部数据源相结合，来提升模型的输出质量。

从外部的数据库中检索出Context，并将这些信息连同用户的问题一起传递给(LLM)大语言模型,从而提高输出的准确性和可靠性。
## RAG流程
### RAG工作原理
- 1.当用户提出问题时，系统先将问题转换为向量表示，随后在向量数据库中进行相似性检索。
- 2.向量数据库中储存的是外部知识库信息(公司内部产品信息、特定项目的专属资料)，纯向量数据库存储的并非大量的外部知识库原始内容，而是经过一系处理后得出的向量数据。
- 3.当系统检索出相关信息后，作为问题的上下文相关信息(Context)来使用(向量数据库存储的是向量数据和数据片段)。
- 4.向量数据库的Context信息和用户的问题会被嵌入到提示词模板中。与上下文相关信息相结合，形成一个全新的提示词。
- 5.新的提示词会被发送到大语言模型(GPT4、Claude)，利用其强大的推理和文本生成能力，生成一个答案。
- ![RAG流程简化版.png](images/RAG%E6%B5%81%E7%A8%8B%E7%AE%80%E5%8C%96%E7%89%88.png)

### RAG的优点
- 1.减少大模型幻觉问题。
- 2.RAG能够为大语言模型注入最新资讯及特定领域的关键信息。帮助模型形成更精准、更贴合的需求和问答。
- 3.在实际应用中，模型微调(fine-tuning)不仅成本高昂，而且每当模型更新时，都需要重新进行一系列复杂过程。相比之下RAG提供了高效低成本的方案。

### TAG流程图
![RAG流程图.png](images/RAG%E6%B5%81%E7%A8%8B%E5%9B%BE.png)

- 1.0.对各种文档PDF、Word、Wiki、Excel等进行分割处理成**文本块(Text Chunk)**
- 1.1.文件分割的质量决定了检索的准确性和生成模型的效果,文本被分割成快的过程，被称为**文本分块**。
- 1.2.文本可以按照段落、固定长度、句子进行分块。
- 2.0.文本块由调用**Embedding Model（嵌入模型）**转换为向量数据。
- 2.1.Embedding Model是一种机器学习模型，可以将文本、图像、转换为低维向量。
- 2.2.在RAG中嵌入模型将文本转换成向量，向量中捕捉了文本的语义信息，从而可以在海量文本库中检索相关内容。
- 2.3.嵌入模型例如:OpenAI的Text-embedding-3-large模型等，但不同模型生成的向量嵌入数值可能会有所不同。
- 2.4.向量嵌入(Vector Embeddings) 是以数值向量的形式表示的数据对象。在多维空间中捕捉文本、图像、音频的语义和关联。能够使机器学习算法更轻松的对其进行处理和解读。
- ![向量数值.png](images/%E5%90%91%E9%87%8F%E6%95%B0%E5%80%BC.png)
- 3.0.通过向量数据库的向量搜索，在向量空间中查找相似对象（距离相近的邻居）。
- 3.1.不同类别的向量在嵌入空间中形成不同的簇，通常的Embedding有数百上千个维度，通过PCA技术将嵌入维度2048维降低到3维，方便可视化显示。
- 3.2.相似的对象，在向量空间中靠的跟进，不相似的则会分散的更远。
- 4.0.Embedding存储在向量数据库中(**Chroma**、Pinecone、Redis)，向量数据库用于处理与相似性搜索相关的任务。
- 4.1.Chroma 本身不负责将文本转换为向量（Embedding）,调用 OpenAI、Cohere 或 Hugging Face 的在线Embedding API
- 4.2.向量数据库可以作为AI系统的长期记忆库。
- 4.3.非结构化数据可以通过语义相似度进行相似性搜索(Similarity Search)，衡量指标主要有欧几里得距离（L2）、余弦相似度、点积(IP)相似度。
- 4.4.通过向量数据库核心算法RNN（近似最近邻），更高效的找到相似邻。
- 5.0.用户的提问的问题，也会被嵌入模型转换成向量嵌入(必须是由同一个嵌入模型计算向量)。系统会讲该向量于向量数据库中的其他向量进行比较，执行相似性搜索。
- 5.1.通过相似度搜索，在向量数据库中找到与用户问题相关的信息的过程，被称为**检索(Retrieval)**。
- 5.2.余弦相似度（-1~1）常用于基于文本的数据。接近1的值表示之间有很高的相似性。
- 6.0.检索阶段，会从海量的文档或数据集中，找到于用户查询最为相关的内容，然后筛选出排名靠前的K个文本片段，Top_K一般是10。
- 7.0.在检索出的Top_K文本片的基础上，通过于用户查询的相关性和上下文适配度进行重新调整。
- 7.1.重新排序的过程叫ReRanking。通过**重排序模型(re-ranking model)**重新进行排序。
- 8.0.在重新排序的文本块中筛选出排名靠前的Top_N个文本,Top_N一般是5。
- 9.0.Top_N作为上下文Context结合用户输入嵌入到提示词模板中，构建出新的提示词，发送给大模型形成最终的答案。
- 9.1.通过ReRanking，更准确的挑选出合适的片段，提升整理相应质量。
- 9.2.提示词模板例如：请根据下面的context(上下文相关信息)回答xxx问题。如果没办法根据提示词回答，请回复“我不知道”。


### 相似度衡量指标
- 欧几里得距离 用于测量高纬空间中两点之间的直线距离
- 余弦相似度 用于计算两个非零向量的夹角的余弦值，常用于基于文本的数据。1表示两个向量完全相同，0表示正交(没有相似性)，-1表示方向相反。
- 点积相似度 用于计算两个向量的模长乘积于他们之间夹角余弦值的乘积，点积会受到向量的长度和方向的影响。